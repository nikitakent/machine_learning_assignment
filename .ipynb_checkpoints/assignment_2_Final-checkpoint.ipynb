{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Environment Requirements'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Environment Requirements'''\n",
    "\n",
    "# Using Python 3.9.6 64-bit\n",
    "# Install libraries\n",
    "\n",
    "# pip3 install os\n",
    "# pip3 install pandas \n",
    "# pip3 install glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import libraries'''\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "# from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maintain the ham:spam ratio (which differs in each folder we have been provided), we have collated all the \"ham\" text files into one large \"ham\" array, and all the spam text files into one large \"spam array\". \n",
    "\n",
    "To achieve this we used the libraries *os* and *glob*: the former sets/changes the working directory, while the latter searches for a specific file pattern. Since our ham/spam folders were nested, glob was really useful......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set paths to download files'''\n",
    "\n",
    "# Set working directory: \n",
    "path = \"C:/Users/64277/OneDrive - AUT University/Data Mining - COMP723/Data Mining Assignment\"\n",
    "\n",
    "# Change the working directory \n",
    "os.chdir(path)\n",
    "\n",
    "ham_spam_files = glob.glob(path + '/enron*/*am/*am.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''Debugging'''\n",
    "# debug - iterating through lists in python *** CAN DELETE LATER ***\n",
    "\n",
    "# firstTen = ham_spam_files[0:10]\n",
    "\n",
    "# # print(firstTen)\n",
    "\n",
    "# n = 0\n",
    "# for file in ham_spam_files: \n",
    "\n",
    "#     if \"enron1\" in file: \n",
    "#         n+=1\n",
    "#     if \"spam\" in file: \n",
    "#         print(\"success\")\n",
    "\n",
    "# print(\"There are\", n, \"text files within enron1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''iterate through all the files Note  this takes 5 to 10 minutes to process'''\n",
    "\n",
    "# loop through list of files and store the output into a dataframe using DataFrame.loc\n",
    "\n",
    "column_names = ([\"category\", \"message\", \"enron folder\"])\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "i = 0\n",
    "\n",
    "for f in ham_spam_files:\n",
    "    # see if enron1, 2, 3, 4, 5: \n",
    "    if \"spam\" in f: \n",
    "        df.loc[i, ['category']] = \"spam\"\n",
    "    if \"ham\" in f: \n",
    "        df.loc[i, ['category']] = \"ham\"\n",
    "    # get the folder \n",
    "    for x in range(1,6): \n",
    "\n",
    "        if (\"enron\" + str(x)) in f: \n",
    "            df.loc[i, ['enron folder']]= \"enron\" + str(x)\n",
    "    # open the file path and read in text\n",
    "    with open(f, 'r', encoding = 'ascii', errors='ignore') as file:\n",
    "        df.loc[i, ['message']] = file.read()\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "#Print result\n",
    "print(f'Successfully created  Temp dataframe for Ham:Spam with shape {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Explore the data'''\n",
    "\n",
    "# print row one of the data frame to check our data\n",
    "print(df.iloc[0])\n",
    "\n",
    "# verify shape one more time\n",
    "print(f'Shape of our dataframe is {df.shape}')\n",
    "\n",
    "# how many words are in the data?\n",
    "\n",
    "print(df['message'].apply(lambda x: len(x.split(' '))).sum())\n",
    "# there are 7 million words in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clean the data'''\n",
    "\n",
    "# check for duplicates and remove them \n",
    "df.drop_duplicates(inplace = True) \n",
    "\n",
    "# show the new shape\n",
    "df.shape\n",
    "\n",
    "# drops 661 rows from our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Clean the data 2.0 '''\n",
    "\n",
    "# show the number of missing data (e.g., \"Nan\",\"NAN\" or \"na\") for each column\n",
    "df.isnull().sum()\n",
    "\n",
    "# no missing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert the Category field from text to an int'''\n",
    "\n",
    "df['label'] = df['category'].apply(lambda x:1 if x=='spam' else 0)\n",
    "\n",
    "# check sample of the data\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split the data into training and testing using train_test_split'''\n",
    "# This is the **baseline** model that we will be using to compare it to the \n",
    "# more advanced models, such as neural networks, naive bayes and random forest. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "emails = df['message'].values\n",
    "y = df['label'].values\n",
    "\n",
    "\n",
    "email_train, email_test, y_train, y_test = train_test_split(\n",
    "  emails, y, test_size=0.3, random_state=1000, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Vectorising using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Bag of words Vectoriser'''\n",
    "\n",
    "# Vectorise the emails with a Bag of Words (BoW) approach using the CountVectorizer from SkLearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#initialise the vectoriser\n",
    "BoWvectorizer = CountVectorizer()\n",
    "\n",
    "# # center the training set - standard score\n",
    "BoW_result= BoWvectorizer.fit(email_train)\n",
    "print(BoW_result)\n",
    "\n",
    "# transform the rest of the data\n",
    "X_train = BoW_result.transform(email_train)\n",
    "X_test = BoW_result.transform(email_test)\n",
    "\n",
    "# print(BoW_result['label'== 0].vocabulary_) - we can see the results of the tokenisation!\n",
    "# type(BoW_result)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on the Bag of Words tokenising model: \n",
    "\n",
    "The resulting feature vectors have 18,938 samples which is also the number of training samples after the train_test split.  \n",
    "Each sample has 114,646 dimensions which is the size of the words in our sample's largest email (i.e., this is the size of our vocabulary)\n",
    "\n",
    "The count vectoriser also gives us a sparse matrix. This is a type of matrices that is less intensive on the processing memory as it only counts non-zero elements in matrices where there are few non-zero elements. \n",
    "\n",
    "Python's CountVectoriser() function tokenises each word in the email message, and it removes special punctuation and other special characters. \n",
    "#TODO: it is possible to use a custom tokeniser with CountVectoriser using the nltk library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Logistic Regression \n",
    "* uses bag of words vectoriser with all default settins used (no max features, no language specification, no stop words used)\n",
    "* special characters have been removed with the default settings in CountVetoriser\n",
    "* duplicate emails have been removed \n",
    "* no special feature extraction performed: very simple baseline model\n",
    "For more information on the function visit: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Logistic Regression'''\n",
    "\n",
    "## Apply Classification Algorithms on the Conflated data  ###\n",
    "## Apply Logistic Regression Algorithm on the data ##\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier_lr = LogisticRegression(class_weight =\"balanced\", max_iter = 1000000)\n",
    "classifier_lr.fit(X_train, y_train)\n",
    "y_pred_lr = classifier_lr.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score using Logistic Regression\n",
    "score = classifier_lr.score(X_test, y_test)\n",
    "print(f\"Accuracy with Logistic Regression: {score * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# The target columns we want to classify\n",
    "target_names = ['Ham', 'Spam']\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score = f1_score(y_test, y_pred_lr)\n",
    "print(f\"F1 score with Logistic Regression: {f1score * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print('Classification report:')\n",
    "report = classification_report(y_test, y_pred_lr,target_names=target_names)\n",
    "print(report)\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print('Logistic Regression Confusion Matrix: \\n', confusion_matrix(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Debugging'''\n",
    "# x = 0\n",
    "# for i in y_test:\n",
    "#     if 1 in y_test:\n",
    "#         x += 1\n",
    "\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Visualisation of confusion matrix for baseline logistic regression classifier'''\n",
    "\n",
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# save confusion matrix as object: \n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "class_names=[1,0] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"rocket\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# I. Deep Neural Networks learning model\n",
    "* uses the logistic activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Neural Networks model using the bag of words pre-processing and 70:30 data split'''\n",
    "\n",
    "# Applying Neural Network model logistic on the data #\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifer_nn = MLPClassifier(activation='logistic', solver='lbfgs', learning_rate_init=0.1, alpha=1e-5,\n",
    "                        hidden_layer_sizes=(5, 2), random_state=1,max_iter=2000)\n",
    "#clf.fit(predicted_train, np.ravel(target_train, order='C'))\n",
    "classifer_nn = classifer_nn.fit(X_train, y_train)\n",
    "y_pred_nn = classifer_nn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score using MLP Neural Network\n",
    "score_nn = classifer_nn.score(X_test, y_test)\n",
    "print(f\"Accuracy with MLP Classifier: {score_nn * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score_nn = f1_score(y_test, y_pred_nn)\n",
    "print(f\"F1 score with MLP Classifier: {f1score_nn * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test,y_pred_nn, target_names = target_names))\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MLP Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred_nn))\n",
    "print(\"\\nEnds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Naive Bayes \n",
    "\n",
    "- multinomial naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Naive Bayes model using the bag of words pre-processing and 70:30 data split'''\n",
    "\n",
    "# Model Generation Using Multinomial Naive Bayes #\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "classifier_nb = MultinomialNB()\n",
    "classifier_nb.fit(X_train, y_train)\n",
    "y_pred_nb = classifier_nb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score using Multinomial Naive Bayes\n",
    "score_nb = classifier_nb.score(X_test, y_test)\n",
    "print(f\"Accuracy with MultinomialNB: {score_nb * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score_nb = f1_score(y_test, y_pred_nb)\n",
    "print(f\"F1 score with MultinomialNB: {f1score_nb * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Classification Report: \") \n",
    "print(classification_report(y_test,y_pred_nb, target_names = target_names))\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MultinomialNB Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred_nb))\n",
    "print(\"\\nEnds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Visualisation of confusion Matrix for Naive Bayes'''\n",
    "\n",
    "# save confusion matrix as object: \n",
    "cnf_matrix = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "# initialise plot\n",
    "class_names=[1,0] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"rocket\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Random Forest model using the bag of words pre-processing and 70:30 data split'''\n",
    "\n",
    "# # Apply Random Forest algorithm on the data - by far the slowest algorithm! #\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# classifier_rf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "# classifier_rf.fit(X_train, y_train)\n",
    "# y_pred_rf = classifier_rf.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy score using Random Forest\n",
    "# score_rf = classifier_rf.score(X_test, y_test)\n",
    "# print(f\"Accuracy with Random Forest: {score_rf * 100}% \")\n",
    "# print(\"*************************************************************\")\n",
    "\n",
    "# # Get performance of algorithm with F score\n",
    "# f1score_rf = f1_score(y_test, y_pred_rf)\n",
    "# print(f\"F1 score with Random Forest: {f1score_rf * 100}% \")\n",
    "# print(\"*************************************************************\")\n",
    "\n",
    "# # Get classification stats\n",
    "# print('Classification report: ')\n",
    "# print(classification_report(y_test,y_pred_rf, target_names = target_names))\n",
    "# print(\"*************************************************************\")\n",
    "\n",
    "# # Get confusion matrix\n",
    "# print(\"Random Forest Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Using a second base line model - lets test the classifiers using this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Second baseline model using enron 1, 3, 5  as training data and enron 2, 4 as test data'''\n",
    "\n",
    "# split the data using dataframe extraction techniques\n",
    "x_train_df = df.loc[(df['enron folder'] == \"enron1\") |(df['enron folder'] == \"enron3\" ) | (df['enron folder'] == \"enron5\")]\n",
    "x_train2 = x_train_df['message'].values # training email messages\n",
    "x_test_df = df[(df['enron folder'] == \"enron2\") |(df['enron folder'] == \"enron4\" )] \n",
    "x_test2 = x_test_df['message'] # testing email messages\n",
    "y_train_df = df[(df['enron folder'] == \"enron1\") |(df['enron folder'] == \"enron3\" ) | (df['enron folder'] == \"enron5\")] \n",
    "y_train2 = y_train_df['label']# training labels\n",
    "y_test_df = df[(df['enron folder'] == \"enron2\") |(df['enron folder'] == \"enron4\" )] \n",
    "y_test2 = y_test_df['label']# testing labels\n",
    "\n",
    "# debug to make sure this has worked\n",
    "# print(y_test[:10])\n",
    "# print(y_test[-10:])\n",
    "# print(x_train[:10])\n",
    "# print(x_train[-10:])\n",
    "#  # etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Vectorise the emails with a Bag of Words (BoW) approach using the CountVectorizer from SkLearn'''\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialise the vectoriser and fit the data\n",
    "vectorizer = CountVectorizer()\n",
    "vec_fit =vectorizer.fit(x_train2)\n",
    "\n",
    "# transform the data\n",
    "x_train2 = vec_fit.transform(x_train2)\n",
    "x_test2  = vec_fit.transform(x_test2)\n",
    "\n",
    "# view shape of the sparse matrix\n",
    "x_train2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "classifier_lr2 = LogisticRegression(class_weight =\"balanced\", max_iter = 1000000)\n",
    "classifier_lr2.fit(x_train2, y_train2)\n",
    "y_pred_lr2 = classifier_lr2.predict(x_test2)\n",
    "\n",
    "# Calculate accuracy score using Logistic Regression\n",
    "score_lr2 = classifier_lr2.score(x_test2, y_test2)\n",
    "print(f\"Accuracy with Logistic Regression - Enron Folder Split: {score_lr2 * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# The target columns we want to classify\n",
    "target_names = ['Ham', 'Spam']\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score_lr2 = f1_score(y_test2, y_pred_lr2)\n",
    "print(f\"F1 score with Logistic Regression - Enron Folder Split: {f1score_lr2 * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print('Classification report - Enron Folder Split:')\n",
    "report = classification_report(y_test2, y_pred_lr2,target_names=target_names)\n",
    "print(report)\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print('Logistic Regression Confusion Matrix - Enron Folder Split: \\n', confusion_matrix(y_test2, y_pred_lr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Visualisation of confusion Matrix for Logistic Regression Part II'''\n",
    "\n",
    "# save confusion matrix as object: \n",
    "cnf_matrix = confusion_matrix(y_test2, y_pred_lr2)\n",
    "\n",
    "class_names=[1,0] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"rocket\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Multinomial Naive Bayes model using the BoW pre-processing and manual split by enron folder'''\n",
    "\n",
    "# Model Generation Using Multinomial Naive Bayes #\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "classifier_nb2 = MultinomialNB()\n",
    "classifier_nb2.fit(x_train2, y_train2)\n",
    "y_pred_nb2 = classifier_nb2.predict(x_test2)\n",
    "\n",
    "# Calculate accuracy score using Multinomial Naive Bayes\n",
    "score_nb2 = classifier_nb2.score(x_test2, y_test2)\n",
    "print(f\"Accuracy with MultinomialNB - Enron Folder Split: {score_nb2 * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score_nb2 = f1_score(y_test2, y_pred_nb2)\n",
    "print(f\"F1 score with MultinomialNB - Enron Folder Split: {f1score_nb2 * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Classification Report - Enron Folder Split: \") \n",
    "print(classification_report(y_test2,y_pred_nb2, target_names = target_names))\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MultinomialNB Confusion Matrix - Enron Folder Split: \\n\",confusion_matrix(y_test2,y_pred_nb2))\n",
    "print(\"\\nEnds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Visualisation of confusion Matrix for Naive Bayes Part II'''\n",
    "\n",
    "# save confusion matrix as object: \n",
    "cnf_matrix = confusion_matrix(y_test2, y_pred_nb2)\n",
    "\n",
    "class_names=[1,0] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"rocket\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Neural Networks - Multi Layer Pereceptron (MLP) Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Neural Networks model using the bag of words pre-processing and manual split by enron folder'''\n",
    "\n",
    "# Applying Neural Network model logistic on the data #\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifer_nn2 = MLPClassifier(activation='logistic', solver='lbfgs', learning_rate_init=0.1, alpha=1e-5,\n",
    "                        hidden_layer_sizes=(5, 2), random_state=1,max_iter=2000)\n",
    "# clf.fit(predicted_train, np.ravel(target_train, order='C'))\n",
    "classifer_nn2 = classifer_nn2.fit(x_train2, y_train2)\n",
    "y_pred_nn2 = classifer_nn2.predict(x_test2)\n",
    "\n",
    "# Calculate accuracy score using MLP Neural Network\n",
    "score_nn2 = classifer_nn2.score(x_test2, y_test2)\n",
    "print(f\"Accuracy with MLP Classifier - Enron Folder Split: {score_nn2 * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score_nn2 = f1_score(y_test2, y_pred_nn2)\n",
    "print(f\"F1 score with MLP Classifier - Enron Folder Split: {f1score_nn2 * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Classification Report - Enron Folder Split: \")\n",
    "print(classification_report(y_test2,y_pred_nn2, target_names = target_names))\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MLP Confusion Matrix - Enron Folder Split: \\n\",confusion_matrix(y_test2,y_pred_nn2))\n",
    "print(\"\\nEnds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Random Forest model using the bag of words pre-processing and 70:30 data split'''\n",
    "\n",
    "# # Apply Random Forest algorithm on the data - by far the slowest algorithm! #\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# classifier_rf2 = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "# classifier_rf2.fit(x_train2, y_train2)\n",
    "# y_pred_rf2 = classifier_rf2.predict(x_test2)\n",
    "\n",
    "# # Calculate accuracy score using Random Forest\n",
    "# score_rf2 = classifier_rf2.score(x_test2, y_test2)\n",
    "# print(f\"Accuracy with Random Forest - Enron Folder Split: {score_rf2 * 100}% \")\n",
    "# print(\"*************************************************************\")\n",
    "\n",
    "# # Get performance of algorithm with F score\n",
    "# f1score_rf2 = f1_score(y_test2, y_pred_rf2)\n",
    "# print(f\"F1 score with Random Forest - Enron Folder Split: {f1score_rf2 * 100}% \")\n",
    "# print(\"*************************************************************\")\n",
    "\n",
    "# # Get classification stats\n",
    "# print('Classification report - Enron Folder Split: ')\n",
    "# print(classification_report(y_test2,y_pred_rf2, target_names = target_names))\n",
    "# print(\"*************************************************************\")\n",
    "\n",
    "# # Get confusion matrix\n",
    "# print(\"Random Forest Confusion Matrix - Enron Folder Split: \\n\", confusion_matrix(y_test2, y_pred_rf2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Using a TF-IFD vectoriser on the top performing classifiers. Does the vectoriser make a difference to our results?\n",
    "\n",
    "In the previous chunks we have been using the CountVectorizer() function (Bag of Words), but since our accuracy and recall has dropped in the [enron1, enron5, enron3]:[enron2, enron4] data - compared to the 70:30 split data, it might be worthwhile to try another type of vectoriser to see if it improves. \n",
    "\n",
    "## Explanation of TF-IDF\n",
    "\n",
    "Using TF-IDF instead of BOW, TF-IDF also takes into account the frequency instead of just the occurrence.\n",
    "This is calculated as:\n",
    "\n",
    "* Term frequency = (Number of Occurrences of a word)/(Total words in the document)**\n",
    "\n",
    "* IDF(word) = Log((Total number of documents)/(Number of documents containing the word))** \n",
    "\n",
    "* TF-IDF is the product of the two.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Vectorise the data using the TfidfTransformer''' \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tvvectoriser=TfidfVectorizer(max_df=0.10,min_df=2,stop_words='english')\n",
    "\n",
    "message_values = df['message'].values\n",
    "vectors = tvvectoriser.fit_transform(message_values)\n",
    "\n",
    "'''Split the data using 70:30 split''' \n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(\n",
    "  vectors, y, test_size=0.3, random_state=1000, stratify = y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Multinomial Naive Bayes model using the tfidf vectorisation and 70:30 split'''\n",
    "\n",
    "# Model Generation Using Multinomial Naive Bayes #\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "# from sklearn import metrics\n",
    "classifier_nb3 = MultinomialNB()\n",
    "classifier_nb3.fit(x_train3, y_train3)\n",
    "y_pred_nb3 = classifier_nb3.predict(x_test3)\n",
    "\n",
    "# Calculate accuracy score using Multinomial Naive Bayes\n",
    "score_nb3 = classifier_nb3.score(x_test3, y_test3)\n",
    "print(f\"Accuracy with MultinomialNB - 70:30 split with TFIDF: {score_nb3 * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score_nb3 = f1_score(y_test3, y_pred_nb3)\n",
    "print(f\"F1 score with MultinomialNB - 70:30 split with TFIDF: {f1score_nb3 * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Classification Report - 70:30 split with TFIDF: \") \n",
    "print(classification_report(y_test3,y_pred_nb3, target_names = target_names))\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MultinomialNB Confusion Matrix - 70:30 split with TFIDF: \\n\",confusion_matrix(y_test3, y_pred_nb3))\n",
    "print(\"\\nEnds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MLP Classifier with TF_IDF and 70:30 split'''\n",
    "\n",
    "# Applying Neural Network model logistic on the data #\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifer_nn3 = MLPClassifier(activation='logistic', solver='lbfgs', learning_rate_init=0.1, alpha=1e-5,\n",
    "                        hidden_layer_sizes=(5, 2), random_state=1,max_iter=2000)\n",
    "# clf.fit(predicted_train, np.ravel(target_train, order='C'))\n",
    "classifer_nn3 = classifer_nn3.fit(x_train3, y_train3)\n",
    "y_pred_nn3 = classifer_nn3.predict(x_test3)\n",
    "\n",
    "# Calculate accuracy score using MLP Neural Network\n",
    "score_nn3_b = classifer_nn3.score(x_test3, y_test3)\n",
    "print(f\"Accuracy with MLP Classifier - Enron Folder Split with TFIDF: {score_nn3_b * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score_nn3_b = f1_score(y_test3, y_pred_nn3)\n",
    "print(f\"F1 score with MLP Classifier - Enron Folder Split with TFIDF: {f1score_nn3_b * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Classification Report - Enron Folder Split with TFIDF: \")\n",
    "print(classification_report(y_test3, y_pred_nn3, target_names = target_names))\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MLP Confusion Matrix - Enron Folder Split with TFIDF: \\n\",confusion_matrix(y_test3, y_pred_nn3))\n",
    "print(\"\\nEnds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Second baseline model using enron 1, 3, 5  as training data and enron 2, 4 as test data'''\n",
    "\n",
    "# import library (if haven't already)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# need to use max features otherwise the sparse matrices have different lengths and this is not suitable for our classifying model\n",
    "tvvectoriser=TfidfVectorizer(max_df=0.95,min_df=2, max_features = 50, stop_words = 'english')\n",
    "\n",
    "# split the data using dataframe extraction techniques\n",
    "x_train_df = df.loc[(df['enron folder'] == \"enron1\") |(df['enron folder'] == \"enron3\" ) | (df['enron folder'] == \"enron5\")]\n",
    "x_train3_b = x_train_df['message'].values # training email messages\n",
    "x_test_df = df[(df['enron folder'] == \"enron2\") |(df['enron folder'] == \"enron4\" )] \n",
    "x_test3_b = x_test_df['message'].values # testing email messages\n",
    "y_train_df = df[(df['enron folder'] == \"enron1\") |(df['enron folder'] == \"enron3\" ) | (df['enron folder'] == \"enron5\")] \n",
    "y_train3_b = y_train_df['label'].values # training labels\n",
    "y_test_df = df[(df['enron folder'] == \"enron2\") |(df['enron folder'] == \"enron4\" )] \n",
    "y_test3_b = y_test_df['label'].values # testing labels\n",
    "\n",
    "# # debug\n",
    "# print(len(x_train3_b))\n",
    "# print(len(y_train3_b))\n",
    "# print(len(x_test3_b))\n",
    "# print(len(y_test3_b))\n",
    "\n",
    "# all the same lengths?\n",
    "\n",
    "'''Vectorise using Tfidf method'''\n",
    "\n",
    "# messages = df['message'].values\n",
    "x_train3_b = tvvectoriser.fit_transform(x_train3_b)\n",
    "x_test3_b = tvvectoriser.fit_transform(x_test3_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Multinomial Naive Bayes model using the tfidf vectorisation and manual enron folder split'''\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Model Generation Using Multinomial Naive Bayes #\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# Import scikit-learn metrics module for accuracy calculation\n",
    "# from sklearn import metrics\n",
    "classifier_nb3_b = MultinomialNB()\n",
    "classifier_nb3_b.fit(x_train3_b, y_train3_b)\n",
    "y_pred_nb3_b = classifier_nb3_b.predict(x_test3_b)\n",
    "\n",
    "# Calculate accuracy score using Multinomial Naive Bayes\n",
    "score_nb3_b = classifier_nb3_b.score(x_test3_b, y_test3_b)\n",
    "print(f\"Accuracy with MultinomialNB - Enron Folder split with TFIDF: {score_nb3_b * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score_nb3_b = f1_score(y_test3_b, y_pred_nb3_b)\n",
    "print(f\"F1 score with MultinomialNB - Enron Folder split with TFIDF: {f1score_nb3_b * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Classification Report - Enron Folder split with TFIDF: \") \n",
    "print(classification_report(y_test3_b, y_pred_nb3_b, target_names = target_names))\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MultinomialNB Confusion Matrix - Enron Folder split with TFIDF: \\n\",confusion_matrix(y_test3_b, y_pred_nb3_b))\n",
    "print(\"\\nEnds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MLP Neural Network classifier model using the tfidf vectorisation and manual enron folder split'''\n",
    "\n",
    "# Applying Neural Network model logistic on the data #\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifer_nn3_b = MLPClassifier(activation='logistic', solver='lbfgs', learning_rate_init=0.1, alpha=1e-5,\n",
    "                        hidden_layer_sizes=(5, 2), random_state=1,max_iter=2000)\n",
    "# clf.fit(predicted_train, np.ravel(target_train, order='C'))\n",
    "classifer_nn3_b = classifer_nn3_b.fit(x_train3_b, y_train2)\n",
    "y_pred_nn3_b = classifer_nn3_b.predict(x_test3_b)\n",
    "\n",
    "# Calculate accuracy score using MLP Neural Network\n",
    "score_nn3_b = classifer_nn3_b.score(x_test3_b, y_test3_b)\n",
    "print(f\"Accuracy with MLP Classifier - Enron Folder Split with TFIDF: {score_nn3_b * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get performance of algorithm with F score\n",
    "f1score_nn3_b = f1_score(y_test3_b, y_pred_nn3_b)\n",
    "print(f\"F1 score with MLP Classifier - Enron Folder Split with TFIDF: {f1score_nn3_b * 100}% \")\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Classification Report - Enron Folder Split with TFIDF: \")\n",
    "print(classification_report(y_test3_b, y_pred_nn3_b, target_names = target_names))\n",
    "print(\"*************************************************************\")\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MLP Confusion Matrix - Enron Folder Split with TFIDF: \\n\",confusion_matrix(y_test3_b, y_pred_nn3_b))\n",
    "print(\"\\nEnds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix (Code not utilised in assessment)\n",
    "\n",
    "## Neural Networks using Tensorflow & Keras\n",
    "We want to use the sigmoid function for the output layer since we are dealing with a binary classification problem\n",
    "if it were a multi-label classification problem then we would want to use the softmax function for the output layer\n",
    "\n",
    "weights most important part of a neural network - start with random values and then uses the backpropogation method. \n",
    "\n",
    "Dependencies: \n",
    "- **pip install keras**\n",
    "- **pip install tensorflow**\n",
    "\n",
    "Note: **pip install tensorflow** might not work so try **pip install tensorflow --user** in Windows \n",
    "\n",
    "\n",
    "The default settings include: \n",
    "\n",
    "    - \"image_data_format\": \"channels_last\",\n",
    "    - \"epsilon\": 1e-07,\n",
    "    - \"floatx\": \"float32\",\n",
    "    - \"backend\": \"tensorflow\n",
    "\n",
    "(These can be found in the json file, enter: \"%USERPROFILE%/.keras/keras.json\" into command prompt on windows, \n",
    "or $HOME/.keras/keras.json on mac. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check tensorflow version: \n",
    "# import tensorflow as tf\n",
    "# tf.__version__\n",
    "\n",
    "# # using version 2.6.1\n",
    "# from keras.models import Sequential\n",
    "# from keras import layers\n",
    "\n",
    "# input_dim = X_train.shape[1]  # Number of features\n",
    "# model = Sequential()\n",
    "# model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ***************************************************************\n",
    "\n",
    "# # Summary of Keras NN model\n",
    "# model.compile(loss='binary_crossentropy', \n",
    "#              optimizer='adam', \n",
    "#             metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# ***************************************************************\n",
    "\n",
    "\n",
    "# Fit the NN Model to the data: \n",
    "# X_train[:15380]\n",
    "# print(X_train) #  (18937, 113176)\n",
    "# # print(y_train) # length 15380, 27,715)\n",
    "# history = model.fit(X_train[:15380], y_train,\n",
    "#                     epochs=100,\n",
    "#                     verbose=False,\n",
    "#                     validation_data=(X_test[:15380], y_test),\n",
    "#                     batch_size=10)\n",
    "\n",
    "# X_train.getnnz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy library for lemmatisation etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import spaCy and load the language library\n",
    "# import spacy\n",
    "# #you will need this line below to download the package\n",
    "# #!python -m spacy download en_core_web_sm\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Convert dataframe to string\n",
    "# #df['message']= df['message'].apply(str)\n",
    "\n",
    "# # Create a Doc object of df\n",
    "# doc = nlp(df[0, 'message'].values)\n",
    "# token_list = []\n",
    "# # collect each token separately with their POS Tag, dependencies and lemma\n",
    "# for token in doc:\n",
    "#     output = [token.text, token.pos_, token.dep_,token.lemma_]\n",
    "#     token_list.append(output)\n",
    "# # create DataFrame using data \n",
    "# df_tokenised = pd.DataFrame(token_list, columns =['Word', 'POS Tag', 'Dependencies', 'Lemmatized Word']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e4161d183124531bcd79d9c6a605b5a829023452995e949ad1a49575a2e0088"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
