{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Python 3.9.6 64-bit\n",
    "# Install libraries\n",
    "\n",
    "# pip3 install os\n",
    "# pip3 install pandas \n",
    "# pip3 install glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "# from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maintain the ham:spam ratio (which differs in each folder we have been provided), we have collated all the \"ham\" text files into one large \"ham\" array, and all the spam text files into one large \"spam array\". \n",
    "\n",
    "To achieve this we used the libraries *os* and *glob*: the former sets/changes the working directory, while the latter searches for a specific file pattern. Since our ham/spam folders were nested, glob was really useful......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory: \n",
    "path = \"C:/Users/64277/OneDrive - AUT University/Data Mining - COMP723/Data Mining Assignment\"\n",
    "\n",
    "# Change the working directory \n",
    "os.chdir(path)\n",
    "\n",
    "ham_spam_files = glob.glob(path + '/enron*/*am/*am.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug - iterating through lists in python *** CAN DELETE LATER ***\n",
    "\n",
    "# firstTen = ham_spam_files[0:10]\n",
    "\n",
    "# # print(firstTen)\n",
    "\n",
    "# n = 0\n",
    "# for file in ham_spam_files: \n",
    "\n",
    "#     if \"enron1\" in file: \n",
    "#         n+=1\n",
    "#     if \"spam\" in file: \n",
    "#         print(\"success\")\n",
    "\n",
    "# print(\"There are\", n, \"text files within enron1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created  Temp dataframe for Ham:Spam with shape (27716, 3)\n"
     ]
    }
   ],
   "source": [
    "'''Iterate through all the files'''\n",
    "# print(enron_folder)\n",
    "\n",
    "# loop through list of files and store the output into a dataframe using DataFrame.loc\n",
    "i = 0\n",
    "\n",
    "column_names = [\"category\", \"message\", \"enron folder\"]\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "for f in ham_spam_files:\n",
    "    # see if enron1, 2, 3, 4, 5: \n",
    "    if \"spam\" in f: \n",
    "        df.loc[i, ['category']] = \"spam\"\n",
    "    if \"ham\" in f: \n",
    "        df.loc[i, ['category']] = \"ham\"\n",
    "    # get the folder \n",
    "    for x in range(1,6): \n",
    "\n",
    "        if (\"enron\" + str(x)) in f: \n",
    "            df.loc[i, ['enron folder']]= \"enron\" + str(x)\n",
    "    # open the file path and read in text\n",
    "    with open(f, 'r', encoding = 'ascii', errors='ignore') as file:\n",
    "        df.loc[i, ['message']] = file.read()\n",
    "\n",
    "    # append temp_df to the end of the main dataframe\n",
    "    # main_df.append(temp_df)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "#Print result\n",
    "print(f'Successfully created  Temp dataframe for Ham:Spam with shape {df.shape}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category                                            ham\n",
      "message         Subject: christmas tree farm pictures\\n\n",
      "enron folder                                     enron1\n",
      "Name: 0, dtype: object\n",
      "Shape of our dataframe is (27716, 3)\n",
      "8109764\n"
     ]
    }
   ],
   "source": [
    "''' Explore the data'''\n",
    "# print row one of the data frame to check our data\n",
    "print(df.iloc[0])\n",
    "\n",
    "# verify shape one more time\n",
    "print(f'Shape of our dataframe is {df.shape}')\n",
    "\n",
    "# how many words are in the data?\n",
    "\n",
    "print(df['message'].apply(lambda x: len(x.split(' '))).sum())\n",
    "# there are 7 million words in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27055, 3)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the data'''\n",
    "\n",
    "# check for duplicates and remove them \n",
    "df.drop_duplicates(inplace = True) \n",
    "\n",
    "# show the new shape\n",
    "df.shape\n",
    "\n",
    "# drops 661 rows from our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category        0\n",
       "message         0\n",
       "enron folder    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Clean the data 2.0 '''\n",
    "# show the number of missing data (e.g., \"Nan\",\"NAN\" or \"na\") for each column\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# no missing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>message</th>\n",
       "      <th>enron folder</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15338</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: l ' m reai | | y surprlsed . . .\\ncii...</td>\n",
       "      <td>enron3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12558</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: article : ipe stays calm over new yor...</td>\n",
       "      <td>enron3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22019</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: doctors use this too 8 cnom\\nci - ial...</td>\n",
       "      <td>enron4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10479</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: are you ready to get it ?\\nhello !\\nv...</td>\n",
       "      <td>enron2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7782</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: projects list\\nstinson ,\\njust want t...</td>\n",
       "      <td>enron2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category                                            message  \\\n",
       "15338     spam  Subject: l ' m reai | | y surprlsed . . .\\ncii...   \n",
       "12558      ham  Subject: article : ipe stays calm over new yor...   \n",
       "22019     spam  Subject: doctors use this too 8 cnom\\nci - ial...   \n",
       "10479     spam  Subject: are you ready to get it ?\\nhello !\\nv...   \n",
       "7782       ham  Subject: projects list\\nstinson ,\\njust want t...   \n",
       "\n",
       "      enron folder  label  \n",
       "15338       enron3      1  \n",
       "12558       enron3      0  \n",
       "22019       enron4      1  \n",
       "10479       enron2      1  \n",
       "7782        enron2      0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert the Category field from text to an int\n",
    "#The models will out a number ranging from 0-1 corresponding to the spam or ham\n",
    "# def convert_category(text):\n",
    "#     if text == \"spam\":\n",
    "#         return 0\n",
    "#     elif text == \"ham\":\n",
    "#         return 1\n",
    "\n",
    "# convert_category(df['category'])\n",
    "\n",
    "df['label'] = df['category'].apply(lambda x:1 if x=='spam' else 0)\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a baseline model\n",
    "# This is the model we will be using to compare it to the more advanced models, such as neural networks, naive bayes and random forest. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "emails = df['message'].values\n",
    "y = df['label'].values\n",
    "\n",
    "\n",
    "email_train, email_test, y_train, y_test = train_test_split(\n",
    "  emails, y, test_size=0.3, random_state=1000, stratify = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<18938x114646 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2214517 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorise the emails with a Bag of Words (BoW) approach using the CountVectorizer from SkLearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BoWvectorizer = CountVectorizer()\n",
    "\n",
    "# center the training set - standard score\n",
    "\n",
    "BoW_result= BoWvectorizer.fit(email_train)\n",
    "print(BoW_result)\n",
    "\n",
    "# transform the rest of the data\n",
    "X_train = BoW_result.transform(email_train)\n",
    "X_test = BoW_result.transform(email_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting feature vectors have 18,938 samples which is also the number of training samples after the train_test split.  \n",
    "Each sample has 114,646 dimensions which is the size of the words in our sample's largest email (i.e., this is the size of our vocabulary)\n",
    "\n",
    "The count vectoriser also gives us a sparse matrix. This is a type of matrices that is less intensive on the processing memory as it only counts non-zero elements in matrices where there are few non-zero elements. \n",
    "\n",
    "Python's CountVectoriser() function tokenises each word in the email message, and it removes special punctuation and other special characters. \n",
    "#TODO: it is possible to use a custom tokeniser with CountVectoriser using the nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 98.79230258792302% \n",
      "Accuracy: 0.9887889614389552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Spam       0.99      0.98      0.99      4371\n",
      "         Ham       0.98      0.99      0.99      3746\n",
      "\n",
      "    accuracy                           0.99      8117\n",
      "   macro avg       0.99      0.99      0.99      8117\n",
      "weighted avg       0.99      0.99      0.99      8117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifier = LogisticRegression(class_weight = \"balanced\", max_iter = 100000000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "## calculate prediction\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "target_names = ['Spam', 'Ham']\n",
    "\n",
    "## get the accuracy and f1 scores\n",
    "\n",
    "# f1 score\n",
    "f1score = f1_score(y_test, y_pred)\n",
    "print(f\"F1 score: {f1score * 100}% \")\n",
    "\n",
    "# accuracy\n",
    "print('Accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "\n",
    "# classification report\n",
    "report = classification_report(y_test, y_pred,target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4304,   67],\n",
       "       [  24, 3722]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifier.show_most_informative_features(20)-- works for naive bayes\n",
    "import math\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# feature_importance = pd.DataFrame(target_names, columns = [\"feature\"])\n",
    "# feature_importance[\"importance\"] = pow(math.e, w)\n",
    "# feature_importance = feature_importance.sort_values(by = [\"importance\"], ascending=False)\n",
    "\n",
    "# rfe = RFE(classifier, 3)\n",
    "# fit = rfe.fit(email_train, email_test)\n",
    "# print(\"Num Features: %d\" % fit.n_features_)\n",
    "# print(\"Selected Features: %s\" % fit.support_)\n",
    "# print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "from sklearn import metrics\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 257.44, 'Predicted label')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visual confusion matrix: \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"rocket\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks learning model\n",
    "\n",
    "we want to use the sigmoid function for the output layer since we are dealing with a binary classification problem\n",
    "if it were a multi-label classification problem then we would want to use the softmax function for the output layer\n",
    "\n",
    "weights most important part of a neural network - start with random values and then uses the backpropogation method. \n",
    "\n",
    "Dependencies: \n",
    "- **pip install keras**\n",
    "- **pip install tensorflow**\n",
    "\n",
    "Note: **pip install tensorflow** might not work so try **pip install tensorflow --user** in Windows \n",
    "\n",
    "\n",
    "The default settings include: \n",
    "\n",
    "    - \"image_data_format\": \"channels_last\",\n",
    "    - \"epsilon\": 1e-07,\n",
    "    - \"floatx\": \"float32\",\n",
    "    - \"backend\": \"tensorflow\n",
    "\n",
    "(These can be found in the json file, enter: \"%USERPROFILE%/.keras/keras.json\" into command prompt on windows, \n",
    "or $HOME/.keras/keras.json on mac. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check tensorflow version: \n",
    "# import tensorflow as tf\n",
    "# tf.__version__\n",
    "\n",
    "# # using version 2.6.1\n",
    "# from keras.models import Sequential\n",
    "# from keras import layers\n",
    "\n",
    "# input_dim = X_train.shape[1]  # Number of features\n",
    "# model = Sequential()\n",
    "# model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 10)                1146470   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,146,481\n",
      "Trainable params: 1,146,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # Summary of Keras NN model\n",
    "# model.compile(loss='binary_crossentropy', \n",
    "#              optimizer='adam', \n",
    "#             metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2214517"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the NN Model to the data: \n",
    "# X_train[:15380]\n",
    "# print(X_train) #  (18937, 113176)\n",
    "# # print(y_train) # length 15380, 27,715)\n",
    "# history = model.fit(X_train[:15380], y_train,\n",
    "#                     epochs=100,\n",
    "#                     verbose=False,\n",
    "#                     validation_data=(X_test[:15380], y_test),\n",
    "#                     batch_size=10)\n",
    "\n",
    "# X_train.getnnz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with Neural Network MLP Classifier: 96.0172228202368% \n",
      "**\n",
      "Accuracy with Neural Network MLP Classifier:  0.9588008565310493\n",
      "**\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96      5827\n",
      "           1       0.93      0.99      0.96      5848\n",
      "\n",
      "    accuracy                           0.96     11675\n",
      "   macro avg       0.96      0.96      0.96     11675\n",
      "weighted avg       0.96      0.96      0.96     11675\n",
      "\n",
      "*\n",
      "Neural Network Confusion Matrix: \n",
      " [[5396  431]\n",
      " [  50 5798]]\n",
      "\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# Applying Neural Network model logistic on the data #\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf1 = MLPClassifier(activation='logistic', solver='lbfgs', learning_rate_init=0.1, alpha=1e-5,\n",
    "                        hidden_layer_sizes=(5, 2), random_state=1,max_iter=2000)\n",
    "#clf.fit(predicted_train, np.ravel(target_train, order='C'))\n",
    "clf1 = clf1.fit(X_train, y_train)\n",
    "predictions = clf1.predict(X_test)\n",
    "\n",
    "# Get performance of algorithm\n",
    "f1score3 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 score with Neural Network MLP Classifier: {f1score3 * 100}% \")\n",
    "print(\"**\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Accuracy with Neural Network MLP Classifier: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"**\")\n",
    "\n",
    "print(\"Classification Report: \", classification_report(y_test,y_pred))\n",
    "print(\"*\")\n",
    "#print(\"Neural Network Classifier Accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"Neural Network Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"\\nEnd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes \n",
    "\n",
    "- multinomial naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with MultinomialNB: 98.9081225033289% \n",
      "**\n",
      "Accuracy with MultinomialNB:  0.9898977454724652\n",
      "**\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4371\n",
      "           1       0.99      0.99      0.99      3746\n",
      "\n",
      "    accuracy                           0.99      8117\n",
      "   macro avg       0.99      0.99      0.99      8117\n",
      "weighted avg       0.99      0.99      0.99      8117\n",
      "\n",
      "**\n",
      "MultinomialNB Confusion Matrix: \n",
      " [[4321   50]\n",
      " [  32 3714]]\n",
      "\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# Model Generation using Multinomial Naive Bayes #\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred= clf.predict(X_test)\n",
    "\n",
    "# Get performance of algorithm\n",
    "f1score2 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 score with MultinomialNB: {f1score2 * 100}% \")\n",
    "print(\"**\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Accuracy with MultinomialNB: \", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"**\")\n",
    "\n",
    "print(\"Classification Report: \", classification_report(y_test,y_pred))\n",
    "print(\"**\")\n",
    "#print(\"MultinomialNB Classifier Accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MultinomialNB Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"\\nEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import spaCy and load the language library\n",
    "# import spacy\n",
    "# #you will need this line below to download the package\n",
    "# #!python -m spacy download en_core_web_sm\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Convert dataframe to string\n",
    "# #df['message']= df['message'].apply(str)\n",
    "\n",
    "# # Create a Doc object of df\n",
    "# doc = nlp(df[0, 'message'].values)\n",
    "# token_list = []\n",
    "# # collect each token separately with their POS Tag, dependencies and lemma\n",
    "# for token in doc:\n",
    "#     output = [token.text, token.pos_, token.dep_,token.lemma_]\n",
    "#     token_list.append(output)\n",
    "# # create DataFrame using data \n",
    "# df_tokenised = pd.DataFrame(token_list, columns =['Word', 'POS Tag', 'Dependencies', 'Lemmatized Word']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Using a second base line model --- lets test the classifiers using this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second baseline model using enron 1, 3, 5  as training data and enron 2, 4 as test data\n",
    "\n",
    "x_train_df = df.loc[(df['enron folder'] == \"enron1\") |(df['enron folder'] == \"enron3\" ) | (df['enron folder'] == \"enron5\")]\n",
    "x_train2 = x_train_df['message'].values # training email messages\n",
    "x_test_df = df[(df['enron folder'] == \"enron2\") |(df['enron folder'] == \"enron4\" )] \n",
    "x_test2 = x_test_df['message'] # testing email messages\n",
    "y_train_df = df[(df['enron folder'] == \"enron1\") |(df['enron folder'] == \"enron3\" ) | (df['enron folder'] == \"enron5\")] \n",
    "y_train2 = y_train_df['label']# training labels\n",
    "y_test_df = df[(df['enron folder'] == \"enron2\") |(df['enron folder'] == \"enron4\" )] \n",
    "y_test2 = y_test_df['label']# testing labels\n",
    "\n",
    "# debug to make sure this has worked\n",
    "# print(y_test[:10])\n",
    "# print(y_test[-10:])\n",
    "# print(x_train[:10])\n",
    "# print(x_train[-10:])\n",
    "#  # etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n"
     ]
    }
   ],
   "source": [
    "# Vectorise the emails with a Bag of Words (BoW) approach using the CountVectorizer from SkLearn\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# ^^^^ this should already be imported from above\n",
    "\n",
    "BoWvectorizer = CountVectorizer()\n",
    "\n",
    "# center the training set - standard score\n",
    "\n",
    "BoW_result2 = BoWvectorizer.fit(x_train2)\n",
    "print(BoW_result2)\n",
    "\n",
    "# transform the rest of the data\n",
    "x_train2 = BoW_result2.transform(x_train2)\n",
    "x_test2 = BoW_result2.transform(x_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 96.0172228202368% \n",
      "Accuracy: 0.9588008565310493\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "classifier = LogisticRegression(class_weight = \"balanced\", max_iter = 100000000)\n",
    "classifier.fit(x_train2, y_train2)\n",
    "\n",
    "## calculate prediction\n",
    "y_pred2 = classifier.predict(x_test2)\n",
    "target_names = ['Spam', 'Ham']\n",
    "\n",
    "## get the accuracy and f1 scores\n",
    "\n",
    "# f1 score\n",
    "f1score = f1_score(y_test2, y_pred2)\n",
    "print(f\"F1 score: {f1score * 100}% \")\n",
    "\n",
    "# accuracy\n",
    "print('Accuracy: %s' % accuracy_score(y_pred2, y_test2))\n",
    "\n",
    "# classification report\n",
    "report = classification_report(y_test2, y_pred2,target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with MultinomialNB: 96.9220475075276% \n",
      "**\n",
      "Accuracy with MultinomialNB:  0.9684796573875804\n",
      "**\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      5827\n",
      "           1       0.95      0.99      0.97      5848\n",
      "\n",
      "    accuracy                           0.97     11675\n",
      "   macro avg       0.97      0.97      0.97     11675\n",
      "weighted avg       0.97      0.97      0.97     11675\n",
      "\n",
      "**\n",
      "MultinomialNB Confusion Matrix: \n",
      " [[5513  314]\n",
      " [  54 5794]]\n",
      "\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# Model Generation using Multinomial Naive Bayes #\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "# from sklearn import metrics\n",
    "clf = MultinomialNB().fit(x_train2, y_train2)\n",
    "y_pred2_nb= clf.predict(x_test2)\n",
    "\n",
    "# Get performance of algorithm\n",
    "f1score2_nb = f1_score(y_test2, y_pred2_nb)\n",
    "print(f\"F1 score with MultinomialNB: {f1score2_nb * 100}% \")\n",
    "print(\"**\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Accuracy with MultinomialNB: \", metrics.accuracy_score(y_test2, y_pred2_nb))\n",
    "print(\"**\")\n",
    "\n",
    "print(\"Classification Report: \", classification_report(y_test2,y_pred2_nb))\n",
    "print(\"**\")\n",
    "#print(\"MultinomialNB Classifier Accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"MultinomialNB Confusion Matrix: \\n\",confusion_matrix(y_test2,y_pred2_nb))\n",
    "print(\"\\nEnd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Neural Networks - Multi Layer Pereceptron (MLP) Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score with Neural Network MLP Classifier: 97.610979329041% \n",
      "**\n",
      "Accuracy with Neural Network MLP Classifier:  0.9758458244111349\n",
      "**\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      5827\n",
      "           1       0.97      0.99      0.98      5848\n",
      "\n",
      "    accuracy                           0.98     11675\n",
      "   macro avg       0.98      0.98      0.98     11675\n",
      "weighted avg       0.98      0.98      0.98     11675\n",
      "\n",
      "**\n",
      "Neural Network Confusion Matrix: \n",
      " [[5632  195]\n",
      " [  87 5761]]\n",
      "\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# Applying Neural Network model logistic on the data #\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf1 = MLPClassifier(activation='logistic', solver='lbfgs', learning_rate_init=0.1, alpha=1e-5,\n",
    "                        hidden_layer_sizes=(5, 2), random_state=1,max_iter=2000)\n",
    "#clf.fit(predicted_train, np.ravel(target_train, order='C'))\n",
    "clf1 = clf1.fit(x_train2, y_train2)\n",
    "y_pred2_nn = clf1.predict(x_test2)\n",
    "\n",
    "# Get performance of algorithm\n",
    "f1score2_nn = f1_score(y_test2, y_pred2_nn)\n",
    "print(f\"F1 score with Neural Network MLP Classifier: {f1score2_nn * 100}% \")\n",
    "print(\"**\")\n",
    "\n",
    "# Get classification stats\n",
    "print(\"Accuracy with Neural Network MLP Classifier: \", metrics.accuracy_score(y_test2, y_pred2_nn))\n",
    "print(\"**\")\n",
    "\n",
    "print(\"Classification Report: \", classification_report(y_test2,y_pred2_nn))\n",
    "print(\"**\")\n",
    "#print(\"Neural Network Classifier Accuracy score: \", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Get confusion matrix\n",
    "print(\"Neural Network Confusion Matrix: \\n\",confusion_matrix(y_test2,y_pred2_nn))\n",
    "print(\"\\nEnd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. TF-IFD vectoriser trial....\n",
    "\n",
    "In the previous chunks we have been using the CountVectorizer() function (Bag of Words), but since our accuracy and recall has dropped in the [enron1, enron5, enron3]:[enron2, enron4] data - compared to the 70:30 split data, it might be worthwhile to try another type of vectoriser to see if it improves. \n",
    "\n",
    "## Explanation of TF-IDF\n",
    "\n",
    "Using TF-IDF instead of BOW, TF-IDF also takes into account the frequency instead of just the occurence.\n",
    "This is calculated as:\n",
    "\n",
    "* Term frequency = (Number of Occurrences of a word)/(Total words in the document)**\n",
    "\n",
    "* IDF(word) = Log((Total number of documents)/(Number of documents containing the word))** \n",
    "\n",
    "* TF-IDF is the product of the two.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TfidfTransformer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12348/2713070622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtfidfconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtfidf_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidfconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# print(tfidf_result.idf_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TfidfTransformer' object is not callable"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tfidfconverter = TfidfTransformer()\n",
    "# tfidf_result = tfidfconverter(x_train2)\n",
    "\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(x_train2)\n",
    "\n",
    "# get feature names: \n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "# print(tfidf_result.idf_)\n",
    "\n",
    "# x_train2_tfidf = tfidf_result.transform(x_train2)\n",
    "# x_test2_tfidf = tfidf_result.transform(x_test2)\n",
    "\n",
    "################\n",
    "\n",
    "# BoWvectorizer = CountVectorizer()\n",
    "\n",
    "# # center the training set - standard score\n",
    "\n",
    "# BoW_result2 = BoWvectorizer.fit(x_train2)\n",
    "# print(BoW_result2)\n",
    "\n",
    "# # transform the rest of the data\n",
    "# x_train2 = BoW_result2.transform(x_train2)\n",
    "# x_test2 = BoW_result2.transform(x_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e4161d183124531bcd79d9c6a605b5a829023452995e949ad1a49575a2e0088"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
